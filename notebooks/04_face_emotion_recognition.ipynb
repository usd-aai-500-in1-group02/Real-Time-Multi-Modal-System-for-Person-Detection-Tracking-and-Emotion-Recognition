{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection & Emotion Recognition using DeepFace\n",
    "\n",
    "This notebook demonstrates face detection and emotion recognition using the DeepFace library.\n",
    "\n",
    "**Objectives:**\n",
    "- Detect faces in images using multiple detection backends\n",
    "- Crop and export face regions\n",
    "- Classify facial expressions into 7 emotion categories\n",
    "- Process videos with face detection and emotion analysis\n",
    "\n",
    "**Library:** [DeepFace](https://github.com/serengil/deepface)\n",
    "\n",
    "**Emotions:** happy, sad, angry, neutral, surprise, fear, disgust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install deepface tf-keras opencv-python matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"../\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"faces\"\n",
    "CROPPED_DIR = OUTPUT_DIR / \"cropped\"\n",
    "EMOTIONS_DIR = OUTPUT_DIR / \"emotions\"\n",
    "\n",
    "# Create directories\n",
    "CROPPED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMOTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"DeepFace version: {DeepFace.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available face detection backends\n",
    "AVAILABLE_BACKENDS = [\n",
    "    'opencv',      # Fast, low accuracy\n",
    "    'ssd',         # Fast, medium accuracy\n",
    "    'mtcnn',       # Medium speed, good accuracy (SELECTED DEFAULT)\n",
    "    'retinaface',  # Slow, best accuracy\n",
    "    'mediapipe',   # Fast, good accuracy\n",
    "    'yolov8',      # Medium speed, good accuracy\n",
    "    'yunet',       # Fast, good accuracy\n",
    "    'centerface'   # Fast, good accuracy\n",
    "]\n",
    "\n",
    "# Default backend (MTCNN - good balance for CPU)\n",
    "DEFAULT_BACKEND = 'mtcnn'\n",
    "\n",
    "# Emotion categories\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "print(f\"Default backend: {DEFAULT_BACKEND}\")\n",
    "print(f\"Emotion categories: {EMOTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample images for testing\n",
    "import urllib.request\n",
    "\n",
    "sample_images = {\n",
    "    \"zidane.jpg\": \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \"bus.jpg\": \"https://ultralytics.com/images/bus.jpg\"\n",
    "}\n",
    "\n",
    "images_dir = DATA_DIR / \"images\"\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename, url in sample_images.items():\n",
    "    filepath = images_dir / filename\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")\n",
    "\n",
    "print(\"\\nSample images ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image_path, detector_backend=DEFAULT_BACKEND, align=True, enforce_detection=False):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using DeepFace.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        detector_backend: Face detection backend to use\n",
    "        align: Whether to align faces\n",
    "        enforce_detection: Raise error if no face detected\n",
    "    \n",
    "    Returns:\n",
    "        face_objs: List of face objects with face image and coordinates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        face_objs = DeepFace.extract_faces(\n",
    "            img_path=str(image_path),\n",
    "            detector_backend=detector_backend,\n",
    "            align=align,\n",
    "            enforce_detection=enforce_detection\n",
    "        )\n",
    "        return face_objs\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting faces: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces in sample image\n",
    "image_path = DATA_DIR / \"images\" / \"zidane.jpg\"\n",
    "print(f\"Detecting faces in: {image_path.name}\")\n",
    "print(f\"Using backend: {DEFAULT_BACKEND}\")\n",
    "\n",
    "face_objs = detect_faces(image_path)\n",
    "\n",
    "print(f\"\\nDetected {len(face_objs)} face(s)\")\n",
    "for i, face in enumerate(face_objs):\n",
    "    area = face['facial_area']\n",
    "    confidence = face.get('confidence', 'N/A')\n",
    "    print(f\"\\nFace {i}:\")\n",
    "    print(f\"  Position: x={area['x']}, y={area['y']}\")\n",
    "    print(f\"  Size: w={area['w']}, h={area['h']}\")\n",
    "    print(f\"  Confidence: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_face_boxes(image, face_objs, color=(0, 255, 0), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes around detected faces.\n",
    "    \n",
    "    Args:\n",
    "        image: Image array (BGR)\n",
    "        face_objs: List of face objects from DeepFace\n",
    "        color: Box color (BGR)\n",
    "        thickness: Line thickness\n",
    "    \n",
    "    Returns:\n",
    "        image: Annotated image\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    for i, face in enumerate(face_objs):\n",
    "        area = face['facial_area']\n",
    "        x, y, w, h = area['x'], area['y'], area['w'], area['h']\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f\"Face {i}\"\n",
    "        cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detected faces\n",
    "original = cv2.imread(str(image_path))\n",
    "annotated = draw_face_boxes(original, face_objs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f\"Detected: {len(face_objs)} face(s)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CROPPED_DIR / \"face_detection_result.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Detection Backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_backends(image_path, backends=['opencv', 'mtcnn', 'retinaface', 'mediapipe']):\n",
    "    \"\"\"\n",
    "    Compare different face detection backends.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to test image\n",
    "        backends: List of backends to compare\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary of backend -> (num_faces, time_taken)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    results = {}\n",
    "    \n",
    "    for backend in backends:\n",
    "        print(f\"Testing {backend}...\", end=\" \")\n",
    "        try:\n",
    "            start = time.time()\n",
    "            faces = detect_faces(image_path, detector_backend=backend)\n",
    "            elapsed = time.time() - start\n",
    "            results[backend] = {\n",
    "                'num_faces': len(faces),\n",
    "                'time_ms': elapsed * 1000,\n",
    "                'faces': faces\n",
    "            }\n",
    "            print(f\"{len(faces)} faces, {elapsed*1000:.0f}ms\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results[backend] = {'num_faces': 0, 'time_ms': 0, 'faces': [], 'error': str(e)}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare backends (using subset for faster testing)\n",
    "print(\"Comparing face detection backends...\\n\")\n",
    "backend_results = compare_backends(\n",
    "    image_path,\n",
    "    backends=['opencv', 'mtcnn', 'retinaface']\n",
    ")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for backend, result in backend_results.items():\n",
    "    comparison_data.append({\n",
    "        'Backend': backend,\n",
    "        'Faces Detected': result['num_faces'],\n",
    "        'Time (ms)': f\"{result['time_ms']:.0f}\"\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nBackend Comparison:\")\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Face Cropping & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_faces(image_path, face_objs, output_dir, base_name=None, padding=20):\n",
    "    \"\"\"\n",
    "    Crop detected faces and save as individual images.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to original image\n",
    "        face_objs: List of face objects from DeepFace\n",
    "        output_dir: Directory to save cropped faces\n",
    "        base_name: Base name for output files\n",
    "        padding: Padding around face region\n",
    "    \n",
    "    Returns:\n",
    "        saved_paths: List of paths to saved face images\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if base_name is None:\n",
    "        base_name = Path(image_path).stem\n",
    "    \n",
    "    # Read original image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    saved_paths = []\n",
    "    \n",
    "    for i, face in enumerate(face_objs):\n",
    "        area = face['facial_area']\n",
    "        x, y, fw, fh = area['x'], area['y'], area['w'], area['h']\n",
    "        \n",
    "        # Add padding\n",
    "        x1 = max(0, x - padding)\n",
    "        y1 = max(0, y - padding)\n",
    "        x2 = min(w, x + fw + padding)\n",
    "        y2 = min(h, y + fh + padding)\n",
    "        \n",
    "        # Crop face\n",
    "        face_crop = image[y1:y2, x1:x2]\n",
    "        \n",
    "        # Save\n",
    "        save_path = output_dir / f\"{base_name}_face_{i}.jpg\"\n",
    "        cv2.imwrite(str(save_path), face_crop)\n",
    "        saved_paths.append(save_path)\n",
    "        \n",
    "        # Also save the aligned face from DeepFace (if available)\n",
    "        if 'face' in face:\n",
    "            aligned_face = face['face']\n",
    "            # Convert from 0-1 float to 0-255 uint8 if needed\n",
    "            if aligned_face.max() <= 1:\n",
    "                aligned_face = (aligned_face * 255).astype(np.uint8)\n",
    "            aligned_path = output_dir / f\"{base_name}_face_{i}_aligned.jpg\"\n",
    "            # DeepFace returns RGB, convert to BGR for OpenCV\n",
    "            if len(aligned_face.shape) == 3:\n",
    "                aligned_bgr = cv2.cvtColor(aligned_face, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(str(aligned_path), aligned_bgr)\n",
    "    \n",
    "    return saved_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop and save faces\n",
    "saved_faces = crop_and_save_faces(\n",
    "    image_path=image_path,\n",
    "    face_objs=face_objs,\n",
    "    output_dir=CROPPED_DIR,\n",
    "    padding=30\n",
    ")\n",
    "\n",
    "print(f\"Saved {len(saved_faces)} cropped face(s):\")\n",
    "for path in saved_faces:\n",
    "    print(f\"  - {path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cropped faces\n",
    "if saved_faces:\n",
    "    n_faces = len(saved_faces)\n",
    "    fig, axes = plt.subplots(1, min(n_faces, 4), figsize=(4 * min(n_faces, 4), 4))\n",
    "    if n_faces == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, face_path in enumerate(saved_faces[:4]):\n",
    "        face_img = cv2.imread(str(face_path))\n",
    "        face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "        axes[i].imshow(face_rgb)\n",
    "        axes[i].set_title(f\"Face {i}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CROPPED_DIR / \"cropped_faces_preview.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotions(image_path, detector_backend=DEFAULT_BACKEND, enforce_detection=False):\n",
    "    \"\"\"\n",
    "    Analyze emotions in faces detected in an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        detector_backend: Face detection backend\n",
    "        enforce_detection: Raise error if no face detected\n",
    "    \n",
    "    Returns:\n",
    "        results: List of analysis results with emotion data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = DeepFace.analyze(\n",
    "            img_path=str(image_path),\n",
    "            actions=['emotion'],\n",
    "            detector_backend=detector_backend,\n",
    "            enforce_detection=enforce_detection\n",
    "        )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing emotions: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze emotions in sample image\n",
    "print(f\"Analyzing emotions in: {image_path.name}\")\n",
    "print(f\"Using backend: {DEFAULT_BACKEND}\")\n",
    "\n",
    "emotion_results = analyze_emotions(image_path)\n",
    "\n",
    "print(f\"\\nAnalyzed {len(emotion_results)} face(s)\")\n",
    "for i, result in enumerate(emotion_results):\n",
    "    print(f\"\\nFace {i}:\")\n",
    "    print(f\"  Dominant emotion: {result['dominant_emotion'].upper()}\")\n",
    "    print(f\"  Emotion scores:\")\n",
    "    for emotion, score in sorted(result['emotion'].items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = '=' * int(score / 5)\n",
    "        print(f\"    {emotion:10s}: {score:5.1f}% {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_emotions(image, emotion_results, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw emotion labels on detected faces.\n",
    "    \n",
    "    Args:\n",
    "        image: Image array (BGR)\n",
    "        emotion_results: Results from DeepFace.analyze\n",
    "        color: Label color (BGR)\n",
    "    \n",
    "    Returns:\n",
    "        image: Annotated image\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # Emotion colors\n",
    "    emotion_colors = {\n",
    "        'happy': (0, 255, 0),      # Green\n",
    "        'sad': (255, 0, 0),        # Blue\n",
    "        'angry': (0, 0, 255),      # Red\n",
    "        'fear': (255, 0, 255),     # Magenta\n",
    "        'surprise': (0, 255, 255), # Yellow\n",
    "        'disgust': (128, 0, 128),  # Purple\n",
    "        'neutral': (128, 128, 128) # Gray\n",
    "    }\n",
    "    \n",
    "    for i, result in enumerate(emotion_results):\n",
    "        region = result['region']\n",
    "        x, y, w, h = region['x'], region['y'], region['w'], region['h']\n",
    "        \n",
    "        dominant = result['dominant_emotion']\n",
    "        score = result['emotion'][dominant]\n",
    "        color = emotion_colors.get(dominant, (0, 255, 0))\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Draw emotion label with background\n",
    "        label = f\"{dominant} ({score:.0f}%)\"\n",
    "        (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(img, (x, y - text_h - 10), (x + text_w + 5, y), color, -1)\n",
    "        cv2.putText(img, label, (x + 2, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emotion analysis\n",
    "original = cv2.imread(str(image_path))\n",
    "emotion_annotated = draw_emotions(original, emotion_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(emotion_annotated, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(\"Emotion Analysis\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EMOTIONS_DIR / \"emotion_analysis_result.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot emotion distribution for first face\n",
    "if emotion_results:\n",
    "    result = emotion_results[0]\n",
    "    emotions = result['emotion']\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_emotions = dict(sorted(emotions.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors = ['#2ecc71' if e == result['dominant_emotion'] else '#3498db' for e in sorted_emotions.keys()]\n",
    "    bars = plt.barh(list(sorted_emotions.keys()), list(sorted_emotions.values()), color=colors)\n",
    "    plt.xlabel('Confidence (%)')\n",
    "    plt.ylabel('Emotion')\n",
    "    plt.title(f\"Emotion Distribution (Dominant: {result['dominant_emotion'].upper()})\")\n",
    "    plt.xlim(0, 100)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, sorted_emotions.values()):\n",
    "        plt.text(value + 1, bar.get_y() + bar.get_height()/2, f'{value:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EMOTIONS_DIR / \"emotion_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_emotions(video_path, output_path, detector_backend=DEFAULT_BACKEND, \n",
    "                           sample_every=1, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process video for face detection and emotion recognition.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_path: Path to save output video\n",
    "        detector_backend: Face detection backend\n",
    "        sample_every: Process every N frames (for speed)\n",
    "        conf_threshold: Minimum confidence for face detection\n",
    "    \n",
    "    Returns:\n",
    "        frame_results: List of per-frame emotion results\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"  Resolution: {width}x{height}, FPS: {fps}\")\n",
    "    print(f\"  Total frames: {total_frames}\")\n",
    "    print(f\"  Processing every {sample_every} frame(s)\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_results = []\n",
    "    last_emotions = None\n",
    "    \n",
    "    for frame_idx in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process every N frames\n",
    "        if frame_idx % sample_every == 0:\n",
    "            try:\n",
    "                results = DeepFace.analyze(\n",
    "                    img_path=frame,\n",
    "                    actions=['emotion'],\n",
    "                    detector_backend=detector_backend,\n",
    "                    enforce_detection=False\n",
    "                )\n",
    "                last_emotions = results\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Draw emotions on frame\n",
    "        if last_emotions:\n",
    "            frame = draw_emotions(frame, last_emotions)\n",
    "            \n",
    "            # Record stats\n",
    "            frame_results.append({\n",
    "                'frame': frame_idx,\n",
    "                'num_faces': len(last_emotions),\n",
    "                'emotions': [r['dominant_emotion'] for r in last_emotions]\n",
    "            })\n",
    "        else:\n",
    "            frame_results.append({\n",
    "                'frame': frame_idx,\n",
    "                'num_faces': 0,\n",
    "                'emotions': []\n",
    "            })\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\nOutput saved to: {output_path}\")\n",
    "    return frame_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available videos\n",
    "video_dir = DATA_DIR / \"videos\"\n",
    "video_extensions = [\".mp4\", \".avi\", \".mov\", \".mkv\"]\n",
    "videos = [f for f in video_dir.iterdir() if f.suffix.lower() in video_extensions] if video_dir.exists() else []\n",
    "\n",
    "if videos:\n",
    "    print(f\"Found {len(videos)} video(s). Processing first one...\")\n",
    "    video_path = videos[0]\n",
    "    output_path = EMOTIONS_DIR / f\"{video_path.stem}_emotions.mp4\"\n",
    "    \n",
    "    frame_results = process_video_emotions(\n",
    "        video_path=video_path,\n",
    "        output_path=output_path,\n",
    "        sample_every=3  # Process every 3rd frame for speed\n",
    "    )\n",
    "    \n",
    "    # Analyze emotion distribution across video\n",
    "    all_emotions = []\n",
    "    for result in frame_results:\n",
    "        all_emotions.extend(result['emotions'])\n",
    "    \n",
    "    if all_emotions:\n",
    "        emotion_counts = pd.Series(all_emotions).value_counts()\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        emotion_counts.plot(kind='bar', color='steelblue')\n",
    "        plt.title('Emotion Distribution Across Video')\n",
    "        plt.xlabel('Emotion')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EMOTIONS_DIR / \"video_emotion_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No videos found in data/videos/\")\n",
    "    print(\"Add video files to test video emotion analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_analyze_emotions(image_folder, detector_backend=DEFAULT_BACKEND):\n",
    "    \"\"\"\n",
    "    Analyze emotions in all images in a folder.\n",
    "    \n",
    "    Args:\n",
    "        image_folder: Path to folder containing images\n",
    "        detector_backend: Face detection backend\n",
    "    \n",
    "    Returns:\n",
    "        all_results: Dictionary of filename -> results\n",
    "    \"\"\"\n",
    "    image_folder = Path(image_folder)\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"]\n",
    "    image_files = [f for f in image_folder.iterdir() if f.suffix.lower() in image_extensions]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    for img_path in tqdm(image_files):\n",
    "        try:\n",
    "            results = analyze_emotions(img_path, detector_backend=detector_backend)\n",
    "            all_results[img_path.name] = {\n",
    "                'num_faces': len(results),\n",
    "                'results': [{\n",
    "                    'dominant_emotion': r['dominant_emotion'],\n",
    "                    'emotion_scores': r['emotion']\n",
    "                } for r in results]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            all_results[img_path.name] = {'num_faces': 0, 'error': str(e)}\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images in data/images folder\n",
    "batch_results = batch_analyze_emotions(DATA_DIR / \"images\")\n",
    "\n",
    "print(\"\\nBatch Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "for filename, result in batch_results.items():\n",
    "    if 'error' in result:\n",
    "        print(f\"{filename}: Error - {result['error']}\")\n",
    "    else:\n",
    "        emotions = [r['dominant_emotion'] for r in result['results']]\n",
    "        print(f\"{filename}: {result['num_faces']} face(s) - {emotions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export batch results to JSON\n",
    "json_path = EMOTIONS_DIR / \"batch_emotion_results.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(batch_results, f, indent=2)\n",
    "print(f\"Results saved to: {json_path}\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_data = []\n",
    "for filename, result in batch_results.items():\n",
    "    if 'results' in result:\n",
    "        for i, face_result in enumerate(result['results']):\n",
    "            row = {\n",
    "                'filename': filename,\n",
    "                'face_id': i,\n",
    "                'dominant_emotion': face_result['dominant_emotion']\n",
    "            }\n",
    "            row.update(face_result['emotion_scores'])\n",
    "            csv_data.append(row)\n",
    "\n",
    "if csv_data:\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = EMOTIONS_DIR / \"batch_emotion_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"CSV saved to: {csv_path}\")\n",
    "    print(\"\\nDataFrame preview:\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze emotions in a custom image\n",
    "# Uncomment and modify the path below:\n",
    "\n",
    "# custom_image = \"/path/to/your/image.jpg\"\n",
    "# \n",
    "# # Detect and analyze\n",
    "# results = analyze_emotions(custom_image)\n",
    "# \n",
    "# # Display results\n",
    "# img = cv2.imread(custom_image)\n",
    "# annotated = draw_emotions(img, results)\n",
    "# \n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "# plt.title(f\"Detected {len(results)} face(s)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# \n",
    "# # Print emotion details\n",
    "# for i, r in enumerate(results):\n",
    "#     print(f\"Face {i}: {r['dominant_emotion']} ({r['emotion'][r['dominant_emotion']]:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Face Detection** using DeepFace with multiple backends (opencv, mtcnn, retinaface, mediapipe)\n",
    "2. **Backend Comparison** - speed vs accuracy tradeoffs\n",
    "3. **Face Cropping** - extracting and saving individual faces\n",
    "4. **Emotion Recognition** - classifying 7 emotions (happy, sad, angry, neutral, surprise, fear, disgust)\n",
    "5. **Video Processing** - frame-by-frame emotion analysis\n",
    "6. **Batch Processing** - analyzing multiple images\n",
    "7. **Results Export** - JSON and CSV output\n",
    "\n",
    "**Key Functions:**\n",
    "- `DeepFace.extract_faces()` - Face detection and cropping\n",
    "- `DeepFace.analyze(actions=['emotion'])` - Emotion recognition\n",
    "\n",
    "**Default Backend:** MTCNN (good balance for CPU)\n",
    "\n",
    "**Next Steps:**\n",
    "- Integrate with person detection (YOLO â†’ face detection within person bbox)\n",
    "- Add face recognition for identity verification\n",
    "- Real-time webcam emotion analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
