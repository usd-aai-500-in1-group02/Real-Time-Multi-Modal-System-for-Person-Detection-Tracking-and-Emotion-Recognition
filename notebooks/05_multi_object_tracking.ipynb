{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Object Tracking using YOLO11 + DeepSort\n",
    "\n",
    "This notebook demonstrates multi-object tracking for person detection using YOLO11 and DeepSort.\n",
    "\n",
    "**Objectives:**\n",
    "- Track multiple persons across video frames with unique IDs\n",
    "- Maintain track identity even during occlusions\n",
    "- Visualize tracking trajectories\n",
    "- Analyze track persistence and lifetime\n",
    "\n",
    "**Models:** \n",
    "- Detection: YOLO11s (small variant - optimized for CPU)\n",
    "- Tracking: DeepSort (Deep Learning + Hungarian Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install ultralytics opencv-python matplotlib pandas tqdm deep-sort-realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"../\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"tracking\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Model: yolo11s.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourangshupal/Downloads/cv-final-project/.venv/lib/python3.12/site-packages/deep_sort_realtime/embedder/embedder_pytorch.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSort tracker initialized\n",
      "  Max age: 30 frames\n",
      "  Init frames: 3\n",
      "  Feature extractor: MobileNet (CPU)\n"
     ]
    }
   ],
   "source": [
    "# Load YOLO11s for detection\n",
    "yolo_model = YOLO(\"yolo11s.pt\")\n",
    "print(f\"YOLO Model: {yolo_model.model_name}\")\n",
    "\n",
    "# Initialize DeepSort tracker\n",
    "tracker = DeepSort(\n",
    "    max_age=30,              # Maximum frames to keep track alive without detections\n",
    "    n_init=3,                # Number of consecutive detections before track is confirmed\n",
    "    nms_max_overlap=1.0,     # NMS threshold\n",
    "    max_cosine_distance=0.3, # Maximum cosine distance for feature matching\n",
    "    nn_budget=None,          # Maximum size of feature gallery\n",
    "    embedder=\"mobilenet\",    # Feature extractor for ReID\n",
    "    embedder_gpu=False       # Use CPU for embeddings\n",
    ")\n",
    "\n",
    "print(f\"DeepSort tracker initialized\")\n",
    "print(f\"  Max age: 30 frames\")\n",
    "print(f\"  Init frames: 3\")\n",
    "print(f\"  Feature extractor: MobileNet (CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_color_for_id(track_id, max_id=100):\n    \"\"\"\n    Generate consistent color for each track ID.\n    \n    Args:\n        track_id: Unique track identifier\n        max_id: Maximum expected ID for color normalization\n    \n    Returns:\n        color: BGR color tuple\n    \"\"\"\n    # Use colormap for consistent colors\n    colormap = cm.get_cmap('hsv')\n    normalized_id = (track_id % max_id) / max_id\n    color_rgb = colormap(normalized_id)[:3]\n    color_bgr = tuple(int(c * 255) for c in reversed(color_rgb))\n    return color_bgr\n\n\ndef draw_tracks(frame, tracks):\n    \"\"\"\n    Draw bounding boxes and IDs for tracks on frame.\n    \n    Args:\n        frame: Input frame\n        tracks: List of track objects from DeepSort\n    \n    Returns:\n        annotated_frame: Frame with track visualizations\n    \"\"\"\n    annotated = frame.copy()\n    \n    for track in tracks:\n        if not track.is_confirmed():\n            continue\n        \n        # Convert track_id to int to avoid string formatting issues\n        track_id = int(track.track_id)\n        ltrb = track.to_ltrb()  # [left, top, right, bottom]\n        \n        x1, y1, x2, y2 = map(int, ltrb)\n        \n        # Get color for this ID\n        color = get_color_for_id(track_id)\n        \n        # Draw bounding box\n        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n        \n        # Draw ID label\n        label = f\"ID: {track_id}\"\n        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        label_y = max(y1 - 10, label_size[1] + 10)\n        \n        # Background for label\n        cv2.rectangle(annotated, \n                     (x1, label_y - label_size[1] - 5), \n                     (x1 + label_size[0] + 5, label_y + 5), \n                     color, -1)\n        \n        # Label text\n        cv2.putText(annotated, label, (x1 + 2, label_y), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n    \n    return annotated\n\n\ndef draw_trajectories(frame, trajectories, max_history=30):\n    \"\"\"\n    Draw trajectory paths for tracks.\n    \n    Args:\n        frame: Input frame\n        trajectories: Dictionary of track_id -> list of (x, y) centers\n        max_history: Maximum number of points to show in trajectory\n    \n    Returns:\n        annotated_frame: Frame with trajectory visualization\n    \"\"\"\n    annotated = frame.copy()\n    \n    for track_id, points in trajectories.items():\n        if len(points) < 2:\n            continue\n        \n        # Get recent points\n        recent_points = points[-max_history:]\n        color = get_color_for_id(track_id)\n        \n        # Draw line segments\n        for i in range(len(recent_points) - 1):\n            pt1 = tuple(map(int, recent_points[i]))\n            pt2 = tuple(map(int, recent_points[i + 1]))\n            \n            # Fade out older points\n            alpha = (i + 1) / len(recent_points)\n            thickness = max(1, int(3 * alpha))\n            \n            cv2.line(annotated, pt1, pt2, color, thickness)\n        \n        # Draw current position as circle\n        if recent_points:\n            current = tuple(map(int, recent_points[-1]))\n            cv2.circle(annotated, current, 4, color, -1)\n    \n    return annotated"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Video Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def track_video(video_path, output_path, conf_threshold=0.25, show_trajectories=True):\n    \"\"\"\n    Track persons in a video file.\n    \n    Args:\n        video_path: Path to input video\n        output_path: Path to save output video\n        conf_threshold: Detection confidence threshold\n        show_trajectories: Whether to draw trajectory paths\n    \n    Returns:\n        tracking_data: Dictionary with tracking statistics\n    \"\"\"\n    video_path = Path(video_path)\n    output_path = Path(output_path)\n    \n    # Open video\n    cap = cv2.VideoCapture(str(video_path))\n    if not cap.isOpened():\n        raise ValueError(f\"Could not open video: {video_path}\")\n    \n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    print(f\"Video: {video_path.name}\")\n    print(f\"  Resolution: {width}x{height}\")\n    print(f\"  FPS: {fps}\")\n    print(f\"  Total frames: {total_frames}\")\n    \n    # Setup video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n    \n    # Tracking data structures\n    trajectories = defaultdict(list)  # track_id -> [(x, y), ...]\n    track_lifetimes = {}  # track_id -> (first_frame, last_frame)\n    frame_counts = []  # persons per frame\n    \n    # Reset tracker for new video\n    tracker_local = DeepSort(\n        max_age=30,\n        n_init=3,\n        nms_max_overlap=1.0,\n        max_cosine_distance=0.3,\n        nn_budget=None,\n        embedder=\"mobilenet\",\n        embedder_gpu=False\n    )\n    \n    for frame_idx in tqdm(range(total_frames), desc=\"Tracking frames\"):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Run YOLO detection\n        results = yolo_model.predict(\n            source=frame,\n            classes=[0],  # Person only\n            conf=conf_threshold,\n            verbose=False\n        )\n        \n        # Extract detections for DeepSort\n        detections = []\n        if results[0].boxes is not None:\n            for box in results[0].boxes:\n                bbox = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]\n                conf = float(box.conf[0])\n                \n                # Convert to [left, top, width, height] for DeepSort\n                x1, y1, x2, y2 = bbox\n                w, h = x2 - x1, y2 - y1\n                detection = ([x1, y1, w, h], conf, 'person')\n                detections.append(detection)\n        \n        # Update tracker\n        tracks = tracker_local.update_tracks(detections, frame=frame)\n        \n        # Count active tracks\n        active_tracks = [t for t in tracks if t.is_confirmed()]\n        frame_counts.append(len(active_tracks))\n        \n        # Update trajectories and lifetimes\n        for track in active_tracks:\n            # Convert track_id to int to avoid string formatting issues\n            track_id = int(track.track_id)\n            \n            # Get center point\n            ltrb = track.to_ltrb()\n            cx = (ltrb[0] + ltrb[2]) / 2\n            cy = (ltrb[1] + ltrb[3]) / 2\n            trajectories[track_id].append((cx, cy))\n            \n            # Update lifetime\n            if track_id not in track_lifetimes:\n                track_lifetimes[track_id] = [frame_idx, frame_idx]\n            else:\n                track_lifetimes[track_id][1] = frame_idx\n        \n        # Draw tracks\n        annotated = draw_tracks(frame, tracks)\n        \n        # Draw trajectories if enabled\n        if show_trajectories:\n            annotated = draw_trajectories(annotated, trajectories)\n        \n        # Add frame info\n        info_text = f\"Frame: {frame_idx+1}/{total_frames} | Tracks: {len(active_tracks)}\"\n        cv2.putText(annotated, info_text, (10, 30), \n                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n        \n        # Write frame\n        out.write(annotated)\n    \n    cap.release()\n    out.release()\n    \n    print(f\"\\nTracking complete!\")\n    print(f\"Output saved to: {output_path}\")\n    \n    return {\n        'frame_counts': frame_counts,\n        'trajectories': dict(trajectories),\n        'track_lifetimes': track_lifetimes,\n        'total_tracks': len(track_lifetimes),\n        'total_frames': total_frames\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 video(s):\n",
      "  - Bangkok.mp4\n"
     ]
    }
   ],
   "source": [
    "# Check for available videos\n",
    "video_dir = DATA_DIR / \"videos\"\n",
    "video_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "video_extensions = [\".mp4\", \".avi\", \".mov\", \".mkv\"]\n",
    "videos = [f for f in video_dir.iterdir() if f.suffix.lower() in video_extensions]\n",
    "\n",
    "if videos:\n",
    "    print(f\"Found {len(videos)} video(s):\")\n",
    "    for v in videos:\n",
    "        print(f\"  - {v.name}\")\n",
    "else:\n",
    "    print(\"No videos found in data/videos/\")\n",
    "    print(\"Please add a video file to test tracking.\")\n",
    "    print(\"\\nYou can download a sample video using:\")\n",
    "    print('  !wget -O ../data/videos/sample.mp4 \"YOUR_VIDEO_URL\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: Bangkok.mp4\n",
      "  Resolution: 1280x720\n",
      "  FPS: 25\n",
      "  Total frames: 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking frames:   0%|          | 2/650 [00:02<13:54,  1.29s/it]/var/folders/cj/13vbmk7n7fqgmdnqjjwn1bx80000gn/T/ipykernel_5753/2816309917.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = cm.get_cmap('hsv')\n",
      "Tracking frames:   0%|          | 2/650 [00:03<17:48,  1.65s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     video_path = videos[\u001b[32m0\u001b[39m]\n\u001b[32m      4\u001b[39m     output_path = OUTPUT_DIR / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_tracked.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tracking_data = \u001b[43mtrack_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_trajectories\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSkipping video processing - no videos available\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mtrack_video\u001b[39m\u001b[34m(video_path, output_path, conf_threshold, show_trajectories)\u001b[39m\n\u001b[32m    100\u001b[39m         track_lifetimes[track_id][\u001b[32m1\u001b[39m] = frame_idx\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Draw tracks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m annotated = \u001b[43mdraw_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Draw trajectories if enabled\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show_trajectories:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mdraw_tracks\u001b[39m\u001b[34m(frame, tracks)\u001b[39m\n\u001b[32m     40\u001b[39m x1, y1, x2, y2 = \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, ltrb)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Get color for this ID\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m color = \u001b[43mget_color_for_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Draw bounding box\u001b[39;00m\n\u001b[32m     46\u001b[39m cv2.rectangle(annotated, (x1, y1), (x2, y2), color, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mget_color_for_id\u001b[39m\u001b[34m(track_id, max_id)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Use colormap for consistent colors\u001b[39;00m\n\u001b[32m     13\u001b[39m colormap = cm.get_cmap(\u001b[33m'\u001b[39m\u001b[33mhsv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m normalized_id = (\u001b[43mtrack_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_id\u001b[49m) / max_id\n\u001b[32m     15\u001b[39m color_rgb = colormap(normalized_id)[:\u001b[32m3\u001b[39m]\n\u001b[32m     16\u001b[39m color_bgr = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(c * \u001b[32m255\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(color_rgb))\n",
      "\u001b[31mTypeError\u001b[39m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "# Process video if available\n",
    "if videos:\n",
    "    video_path = videos[0]\n",
    "    output_path = OUTPUT_DIR / f\"{video_path.stem}_tracked.mp4\"\n",
    "    \n",
    "    tracking_data = track_video(\n",
    "        video_path=video_path,\n",
    "        output_path=output_path,\n",
    "        conf_threshold=0.25,\n",
    "        show_trajectories=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping video processing - no videos available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracking Statistics & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if videos and 'tracking_data' in locals():\n",
    "    # Plot track count over time\n",
    "    frames = list(range(len(tracking_data['frame_counts'])))\n",
    "    counts = tracking_data['frame_counts']\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Subplot 1: Track count over time\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(frames, counts, 'b-', linewidth=1, alpha=0.7)\n",
    "    plt.fill_between(frames, counts, alpha=0.3)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Number of Active Tracks')\n",
    "    plt.title('Active Tracks Over Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Track lifetime distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    lifetimes = [(end - start + 1) for start, end in tracking_data['track_lifetimes'].values()]\n",
    "    plt.hist(lifetimes, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Track Lifetime (frames)')\n",
    "    plt.ylabel('Number of Tracks')\n",
    "    plt.title('Track Lifetime Distribution')\n",
    "    plt.axvline(np.mean(lifetimes), color='r', linestyle='--', \n",
    "                label=f'Mean: {np.mean(lifetimes):.1f} frames')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"tracking_statistics.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n=== Tracking Statistics ===\")\n",
    "    print(f\"Total unique tracks: {tracking_data['total_tracks']}\")\n",
    "    print(f\"Total frames processed: {tracking_data['total_frames']}\")\n",
    "    print(f\"\\nTrack count per frame:\")\n",
    "    print(f\"  Min: {min(counts)}\")\n",
    "    print(f\"  Max: {max(counts)}\")\n",
    "    print(f\"  Average: {np.mean(counts):.2f}\")\n",
    "    print(f\"\\nTrack lifetimes:\")\n",
    "    print(f\"  Min: {min(lifetimes)} frames\")\n",
    "    print(f\"  Max: {max(lifetimes)} frames\")\n",
    "    print(f\"  Average: {np.mean(lifetimes):.1f} frames\")\n",
    "    print(f\"  Median: {np.median(lifetimes):.1f} frames\")\n",
    "else:\n",
    "    print(\"No tracking data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Tracking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if videos and 'tracking_data' in locals():\n",
    "    # Export track lifetimes to CSV\n",
    "    lifetime_data = []\n",
    "    for track_id, (start, end) in tracking_data['track_lifetimes'].items():\n",
    "        lifetime_data.append({\n",
    "            'track_id': track_id,\n",
    "            'first_frame': start,\n",
    "            'last_frame': end,\n",
    "            'lifetime_frames': end - start + 1\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(lifetime_data)\n",
    "    csv_path = OUTPUT_DIR / \"track_lifetimes.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Track lifetimes saved to: {csv_path}\")\n",
    "    print(\"\\nPreview:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # Export frame counts to JSON\n",
    "    json_data = {\n",
    "        'total_tracks': tracking_data['total_tracks'],\n",
    "        'total_frames': tracking_data['total_frames'],\n",
    "        'frame_counts': tracking_data['frame_counts']\n",
    "    }\n",
    "    json_path = OUTPUT_DIR / \"tracking_summary.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    print(f\"\\nTracking summary saved to: {json_path}\")\n",
    "else:\n",
    "    print(\"No tracking data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Model Setup**: Loading YOLO11s for detection and DeepSort for tracking\n",
    "2. **Video Tracking**: Tracking multiple persons with unique IDs across frames\n",
    "3. **Trajectory Visualization**: Drawing path history for each tracked person\n",
    "4. **Track Statistics**: Analyzing track lifetimes and active track counts\n",
    "5. **Data Export**: Saving tracking results to CSV and JSON formats\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Track ID Persistence**: Each person maintains a unique ID across frames\n",
    "- **Occlusion Handling**: Tracker can re-identify persons after brief occlusions\n",
    "- **Feature Matching**: Uses MobileNet features for person re-identification\n",
    "- **Hungarian Algorithm**: Optimally matches detections to existing tracks\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different `max_age` values for better occlusion handling\n",
    "- Try different confidence thresholds to balance precision/recall\n",
    "- Analyze trajectory patterns for behavior recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}