{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Detection using YOLO11\n",
    "\n",
    "This notebook demonstrates person detection using Ultralytics YOLO11.\n",
    "\n",
    "**Objectives:**\n",
    "- Detect and locate all persons in images/video frames\n",
    "- Filter detections to only the \"person\" class\n",
    "- Visualize bounding boxes with confidence scores\n",
    "- Process both images and videos\n",
    "\n",
    "**Model:** YOLO11s (small variant - optimized for CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install ultralytics opencv-python matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path(\"../\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"detections\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11s model (will download automatically on first run)\n",
    "model = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "# Print model info\n",
    "print(f\"Model: {model.model_name}\")\n",
    "print(f\"Task: {model.task}\")\n",
    "print(f\"\\nClass names (showing first 10):\")\n",
    "for i, name in list(model.names.items())[:10]:\n",
    "    print(f\"  {i}: {name}\")\n",
    "print(f\"\\nPerson class ID: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample images for testing\n",
    "import urllib.request\n",
    "\n",
    "sample_images = {\n",
    "    \"bus.jpg\": \"https://ultralytics.com/images/bus.jpg\",\n",
    "    \"zidane.jpg\": \"https://ultralytics.com/images/zidane.jpg\"\n",
    "}\n",
    "\n",
    "images_dir = DATA_DIR / \"images\"\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename, url in sample_images.items():\n",
    "    filepath = images_dir / filename\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(f\"  Saved to {filepath}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists\")\n",
    "\n",
    "print(\"\\nSample images ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_persons(image_path, model, conf_threshold=0.25, save_path=None):\n",
    "    \"\"\"\n",
    "    Detect persons in an image using YOLO11.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        model: YOLO model instance\n",
    "        conf_threshold: Confidence threshold (0-1)\n",
    "        save_path: Optional path to save annotated image\n",
    "    \n",
    "    Returns:\n",
    "        results: Detection results\n",
    "        detections: List of detection dictionaries\n",
    "    \"\"\"\n",
    "    # Run detection (class 0 = person in COCO)\n",
    "    results = model.predict(\n",
    "        source=image_path,\n",
    "        classes=[0],  # Only detect persons\n",
    "        conf=conf_threshold,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Extract detection information\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for i, box in enumerate(boxes):\n",
    "            detection = {\n",
    "                \"id\": i,\n",
    "                \"class\": \"person\",\n",
    "                \"confidence\": float(box.conf[0]),\n",
    "                \"bbox_xyxy\": box.xyxy[0].tolist(),  # [x1, y1, x2, y2]\n",
    "                \"bbox_xywh\": box.xywh[0].tolist(),  # [center_x, center_y, width, height]\n",
    "            }\n",
    "            detections.append(detection)\n",
    "    \n",
    "    # Save annotated image if path provided\n",
    "    if save_path:\n",
    "        annotated = results[0].plot()\n",
    "        cv2.imwrite(str(save_path), annotated)\n",
    "    \n",
    "    return results, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on sample image\n",
    "image_path = DATA_DIR / \"images\" / \"bus.jpg\"\n",
    "save_path = OUTPUT_DIR / \"bus_detected.jpg\"\n",
    "\n",
    "results, detections = detect_persons(image_path, model, save_path=save_path)\n",
    "\n",
    "print(f\"Detected {len(detections)} person(s)\")\n",
    "print(\"\\nDetection details:\")\n",
    "for det in detections:\n",
    "    print(f\"  Person {det['id']}: confidence={det['confidence']:.2f}\")\n",
    "    bbox = det['bbox_xyxy']\n",
    "    print(f\"    Bounding box: x1={bbox[0]:.0f}, y1={bbox[1]:.0f}, x2={bbox[2]:.0f}, y2={bbox[3]:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original and detected images side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original image\n",
    "original = cv2.imread(str(image_path))\n",
    "original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "axes[0].imshow(original)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Detected image\n",
    "detected = cv2.imread(str(save_path))\n",
    "detected = cv2.cvtColor(detected, cv2.COLOR_BGR2RGB)\n",
    "axes[1].imshow(detected)\n",
    "axes[1].set_title(f\"Detected: {len(detections)} person(s)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"comparison_bus.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_folder(folder_path, model, output_folder, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Process all images in a folder and save detection results.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing images\n",
    "        model: YOLO model instance\n",
    "        output_folder: Path to save annotated images and results\n",
    "        conf_threshold: Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        all_results: Dictionary with all detection results\n",
    "    \"\"\"\n",
    "    folder_path = Path(folder_path)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"]\n",
    "    image_files = [f for f in folder_path.iterdir() \n",
    "                   if f.suffix.lower() in image_extensions]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    for image_path in tqdm(image_files):\n",
    "        save_path = output_folder / f\"{image_path.stem}_detected{image_path.suffix}\"\n",
    "        results, detections = detect_persons(image_path, model, conf_threshold, save_path)\n",
    "        \n",
    "        all_results[image_path.name] = {\n",
    "            \"num_persons\": len(detections),\n",
    "            \"detections\": detections\n",
    "        }\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images in the data/images folder\n",
    "batch_results = process_image_folder(\n",
    "    folder_path=DATA_DIR / \"images\",\n",
    "    model=model,\n",
    "    output_folder=OUTPUT_DIR / \"batch\",\n",
    "    conf_threshold=0.25\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nBatch Processing Summary:\")\n",
    "print(\"-\" * 40)\n",
    "total_persons = 0\n",
    "for filename, result in batch_results.items():\n",
    "    print(f\"{filename}: {result['num_persons']} person(s)\")\n",
    "    total_persons += result['num_persons']\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total persons detected: {total_persons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "json_path = OUTPUT_DIR / \"batch_results.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(batch_results, f, indent=2)\n",
    "print(f\"Results saved to {json_path}\")\n",
    "\n",
    "# Save results to CSV\n",
    "csv_data = []\n",
    "for filename, result in batch_results.items():\n",
    "    for det in result['detections']:\n",
    "        bbox = det['bbox_xyxy']\n",
    "        csv_data.append({\n",
    "            \"filename\": filename,\n",
    "            \"person_id\": det['id'],\n",
    "            \"confidence\": det['confidence'],\n",
    "            \"x1\": bbox[0],\n",
    "            \"y1\": bbox[1],\n",
    "            \"x2\": bbox[2],\n",
    "            \"y2\": bbox[3]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(csv_data)\n",
    "csv_path = OUTPUT_DIR / \"batch_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to {csv_path}\")\n",
    "print(f\"\\nDataFrame preview:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Video Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, model, output_path, conf_threshold=0.25, show_progress=True):\n",
    "    \"\"\"\n",
    "    Process video file and save annotated output.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        model: YOLO model instance\n",
    "        output_path: Path to save output video\n",
    "        conf_threshold: Confidence threshold\n",
    "        show_progress: Whether to show progress bar\n",
    "    \n",
    "    Returns:\n",
    "        frame_results: List of detection results per frame\n",
    "    \"\"\"\n",
    "    video_path = Path(video_path)\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video: {video_path.name}\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Total frames: {total_frames}\")\n",
    "    \n",
    "    # Setup video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_results = []\n",
    "    frame_iter = range(total_frames)\n",
    "    if show_progress:\n",
    "        frame_iter = tqdm(frame_iter, desc=\"Processing frames\")\n",
    "    \n",
    "    for frame_idx in frame_iter:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Run detection\n",
    "        results = model.predict(\n",
    "            source=frame,\n",
    "            classes=[0],\n",
    "            conf=conf_threshold,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Get annotated frame\n",
    "        annotated = results[0].plot()\n",
    "        \n",
    "        # Count detections\n",
    "        num_persons = len(results[0].boxes)\n",
    "        frame_results.append({\n",
    "            \"frame\": frame_idx,\n",
    "            \"num_persons\": num_persons\n",
    "        })\n",
    "        \n",
    "        # Write frame\n",
    "        out.write(annotated)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\nOutput saved to: {output_path}\")\n",
    "    return frame_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a sample video, otherwise create a simple test\n",
    "video_dir = DATA_DIR / \"videos\"\n",
    "video_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List available videos\n",
    "video_extensions = [\".mp4\", \".avi\", \".mov\", \".mkv\"]\n",
    "videos = [f for f in video_dir.iterdir() if f.suffix.lower() in video_extensions]\n",
    "\n",
    "if videos:\n",
    "    print(f\"Found {len(videos)} video(s):\")\n",
    "    for v in videos:\n",
    "        print(f\"  - {v.name}\")\n",
    "else:\n",
    "    print(\"No videos found in data/videos/\")\n",
    "    print(\"Please add a video file to test video detection.\")\n",
    "    print(\"\\nYou can download a sample video using:\")\n",
    "    print('  !wget -O ../data/videos/sample.mp4 \"YOUR_VIDEO_URL\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process video if available\n",
    "if videos:\n",
    "    video_path = videos[0]\n",
    "    output_path = OUTPUT_DIR / f\"{video_path.stem}_detected.mp4\"\n",
    "    \n",
    "    frame_results = process_video(\n",
    "        video_path=video_path,\n",
    "        model=model,\n",
    "        output_path=output_path,\n",
    "        conf_threshold=0.25\n",
    "    )\n",
    "    \n",
    "    # Plot detection counts over time\n",
    "    frames = [r['frame'] for r in frame_results]\n",
    "    counts = [r['num_persons'] for r in frame_results]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(frames, counts, 'b-', linewidth=1)\n",
    "    plt.fill_between(frames, counts, alpha=0.3)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Number of Persons')\n",
    "    plt.title('Person Count Over Time')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(OUTPUT_DIR / \"video_detection_plot.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Min persons: {min(counts)}\")\n",
    "    print(f\"  Max persons: {max(counts)}\")\n",
    "    print(f\"  Average: {np.mean(counts):.2f}\")\n",
    "else:\n",
    "    print(\"Skipping video processing - no videos available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence score distribution from batch results\n",
    "confidences = []\n",
    "for result in batch_results.values():\n",
    "    for det in result['detections']:\n",
    "        confidences.append(det['confidence'])\n",
    "\n",
    "if confidences:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(confidences, bins=20, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Confidence Score Distribution')\n",
    "    plt.axvline(np.mean(confidences), color='r', linestyle='--', label=f'Mean: {np.mean(confidences):.2f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(confidences)\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.title('Confidence Score Box Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"confidence_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Confidence Statistics:\")\n",
    "    print(f\"  Min: {min(confidences):.3f}\")\n",
    "    print(f\"  Max: {max(confidences):.3f}\")\n",
    "    print(f\"  Mean: {np.mean(confidences):.3f}\")\n",
    "    print(f\"  Std: {np.std(confidences):.3f}\")\n",
    "else:\n",
    "    print(\"No detections to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Image Detection\n",
    "\n",
    "Use this section to test detection on your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Detect persons in a custom image\n",
    "# Uncomment and modify the path below:\n",
    "\n",
    "# custom_image = \"/path/to/your/image.jpg\"\n",
    "# results, detections = detect_persons(custom_image, model)\n",
    "# \n",
    "# # Display result\n",
    "# annotated = results[0].plot()\n",
    "# annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(annotated_rgb)\n",
    "# plt.title(f\"Detected: {len(detections)} person(s)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Model Loading**: Using YOLO11s for person detection\n",
    "2. **Single Image Detection**: Detecting persons with bounding boxes\n",
    "3. **Batch Processing**: Processing multiple images with results export\n",
    "4. **Video Detection**: Frame-by-frame person detection in videos\n",
    "5. **Results Analysis**: Confidence score distribution analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- Try `02_instance_segmentation.ipynb` for pixel-level person boundaries\n",
    "- Try `03_person_counting.ipynb` for counting and crowd analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
